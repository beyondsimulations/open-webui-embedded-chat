# Required: The API endpoint for the LLM provider
API_ENDPOINT = [Your endpoint]
# Required: The API key for authenticating with the LLM provider
API_KEY = [Your API key]
# Optional: The system prompt to guide the LLM's behavior
SYSTEM_PROMPT = You are a helpful educational assistant.

# Allowed domains for CORS (comma-separated)
# Required: Comma-separated list of allowed domains for CORS
ALLOWED_DOMAINS = beyondsimulations.github.io

# Rate limiting configuration
# Optional: Maximum number of requests allowed in the rate limit window
RATE_LIMIT_REQUESTS = 10
# Optional: Time window for rate limiting in seconds
RATE_LIMIT_WINDOW = 60
# Optional: Burst allowance for rate limiting
RATE_LIMIT_BURST = 3
# Required: Set to "development" or "production" to control logging and behavior
ENVIRONMENT = development

# PostHog configuration for LLM tracing
# Optional: API key for PostHog analytics
POSTHOG_API_KEY = [Your PostHog API key]
# Optional: PostHog endpoint for sending analytics events
POSTHOG_ENDPOINT = https://eu.i.posthog.com/i/v0/e/

# Optional: Specify the AI provider (e.g., openai, anthropic). If not set, it will be derived from the API endpoint.
AI_PROVIDER = openwebui
